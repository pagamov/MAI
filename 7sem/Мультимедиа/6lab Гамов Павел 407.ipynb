{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5U0OQo1ZJAC"
   },
   "source": [
    "PyTorch practice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://download.pytorch.org/whl/cu100/torch_stable.html\n",
      "  Downloading https://download.pytorch.org/whl/cu100/torch_stable.html (62 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Cannot unpack file C:\\Users\\pa-ga\\AppData\\Local\\Temp\\pip-unpack-67d87n3j\\torch_stable.html (downloaded from C:\\Users\\pa-ga\\AppData\\Local\\Temp\\pip-req-build-gmj_zl42, content-type: text/html); cannot detect archive format\n",
      "ERROR: Cannot determine archive format of C:\\Users\\pa-ga\\AppData\\Local\\Temp\\pip-req-build-gmj_zl42\n"
     ]
    }
   ],
   "source": [
    "pip install "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmNTh6Z1ZStq"
   },
   "source": [
    "PyTorch 101 basic knowledges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hTbVELEzZFlm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current PyTorch version: 1.10.0+cpu\n",
      "Random Tensor: \n",
      " tensor([[0.7175, 0.5060, 0.7905],\n",
      "        [0.8864, 0.2155, 0.2649]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f'Current PyTorch version: {torch.__version__}')\n",
    "\n",
    "data = [[1, 2, 3],[4, 5, 6],[7, 8, 9]]\n",
    "x_data = torch.tensor(data)\n",
    "\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "\n",
    "x_ones = torch.ones_like(x_data)\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
    "\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n",
    "\n",
    "tensor = torch.rand(3,4)\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_VaEoKHxLYa"
   },
   "source": [
    "CUDA Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ydPvGtSrZIBL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: False\n",
      "count GPU devices: 0\n",
      "tensor([[-0.4557, -0.3339,  1.2471],\n",
      "        [ 0.2273,  0.6162, -0.1250],\n",
      "        [-0.6488,  0.9938,  0.3043]])\n",
      "tensor([[0, 0, 1],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]], dtype=torch.int32)\n",
      "tensor([[0.0014, 0.6822, 0.2366],\n",
      "        [0.8391, 0.0952, 0.4230],\n",
      "        [0.2955, 0.3145, 0.9373],\n",
      "        [0.9717, 0.2981, 0.7878]], dtype=torch.float64)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f'GPU is available: {torch.cuda.is_available()}')\n",
    "print(f'count GPU devices: {torch.cuda.device_count()}')\n",
    "\n",
    "tensor = torch.randn((3,3))\n",
    "print(tensor)\n",
    "tensor = tensor.to(torch.int32)\n",
    "print(tensor)\n",
    "\n",
    "tensor = torch.rand((4, 3), dtype=torch.double)\n",
    "print(tensor)\n",
    "ten_2 = torch.ones((2,2)).to(tensor)\n",
    "print(ten_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nwEVuvhbHH8V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor is on -1 device\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand((4,3,7,7))\n",
    "print(f'Tensor is on {tensor.get_device()} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Y-dpVMTAHX9b"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3668/2437755348.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Tensor is on {tensor.get_device()} device'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python372\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand((4,3,7,7))\n",
    "tensor = tensor.to('cuda:0')\n",
    "print(f'Tensor is on {tensor.get_device()} device')\n",
    "\n",
    "tensor = torch.rand((4,3,7,7))\n",
    "device = torch.device('cuda')\n",
    "tensor = tensor.to(device)\n",
    "print(f'Tensor is on {tensor.get_device()} device')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "tensor = torch.rand((4,3,7,7))\n",
    "tensor = tensor.to(device)\n",
    "print(f'Tensor is on {tensor.get_device()} device')\n",
    "\n",
    "ten_2 = torch.rand((4,3,7,7)).to(tensor.device)\n",
    "print(f'Tensor is on {tensor.get_device()} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IoU4iBXkHgWr"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4300/1484462253.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mten_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Tensor is on {ten_2.get_device()} device, data type: {ten_2.dtype}, grad: {ten_2.requires_grad}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand((3,2), dtype=torch.double, device=device, requires_grad=True)\n",
    "ten_2 = torch.zeros(2,4).to(tensor)\n",
    "print(f'Tensor is on {ten_2.get_device()} device, data type: {ten_2.dtype}, grad: {ten_2.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uPAdY8qbHPvL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device: <torch.cuda.device_of object at 0x000001BDCD1C2C50>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4300/702679054.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'GPU device: {torch.cuda.device_of(tensor)}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(f'GPU SM_ARCH: {torch.cuda.get_arch_list()}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'GPU capability: {torch.cuda.get_device_capability()}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'GPU properties: {torch.cuda.get_device_properties(0)}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(f'GPU is initialized: {torch.cuda.is_initialized()}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python372\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mget_device_capability\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmajor\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mminor\u001b[0m \u001b[0mcuda\u001b[0m \u001b[0mcapability\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \"\"\"\n\u001b[1;32m--> 342\u001b[1;33m     \u001b[0mprop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python372\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \"\"\"\n\u001b[1;32m--> 356\u001b[1;33m     \u001b[0m_lazy_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# will define _get_device_properties\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python372\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "print(f'GPU device: {torch.cuda.device_of(tensor)}')\n",
    "#print(f'GPU SM_ARCH: {torch.cuda.get_arch_list()}')\n",
    "print(f'GPU capability: {torch.cuda.get_device_capability()}')\n",
    "print(f'GPU properties: {torch.cuda.get_device_properties(0)}')\n",
    "#print(f'GPU is initialized: {torch.cuda.is_initialized()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 6493,
     "status": "error",
     "timestamp": 1638958768366,
     "user": {
      "displayName": "Andriy Sorokin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08930791557530031808"
     },
     "user_tz": -180
    },
    "id": "RgLkxEqnHR6E",
    "outputId": "930af098-9f78-454b-a26c-f971a6588636"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4300/3481420717.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mArch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0moptim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdagrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#print(optim.state_dict())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python372\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32mc:\\python372\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python372\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    591\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python372\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m--> 897\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python372\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "class Arch(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layer1 = torch.nn.Linear(3, 7)\n",
    "    self.layer2 = torch.nn.Linear(7, 1)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = F.tanh(layer1)\n",
    "    x = F.tanh(layer2)\n",
    "    return x\n",
    "\n",
    "\n",
    "model = Arch()\n",
    "model.to('cuda')\n",
    "optim = torch.optim.Adagrad(model.parameters())\n",
    "#print(optim.state_dict())\n",
    "\n",
    "'''\n",
    "That’s not possible. Modules can hold parameters of different types on different \n",
    "devices, and so it’s not always possible to unambiguously determine the device.\n",
    "'''\n",
    "print(f'Model {next(model.parameters()).device}')  \n",
    "for n, p in model.named_parameters():\n",
    "  print(n, p.device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zHtqV_GAR0Zv"
   },
   "outputs": [],
   "source": [
    "class LayerModule_1(torch.nn.Module):\r\n",
    "  def __init__(self):\r\n",
    "    super().__init__()\r\n",
    "    self.layer1 = torch.nn.Linear(4, 8)\r\n",
    "    self.layer2 = torch.nn.Linear(8, 2)\r\n",
    "    \r\n",
    "  def forward(self, x):\r\n",
    "    x = F.tanh(layer1)\r\n",
    "    x = F.tanh(layer2)\r\n",
    "    return x\r\n",
    "\r\n",
    "class LayerModule_2(torch.nn.Module):\r\n",
    "  def __init__(self):\r\n",
    "    super().__init__()\r\n",
    "    self.op = torch.nn.Transformer()\r\n",
    "    \r\n",
    "  def forward(self, x):\r\n",
    "    x = self.op(x)\r\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewArch(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = LayerModule_1()\n",
    "        self.data = LayerModule_2()\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.features(x)\n",
    "      x = self.data(x)\n",
    "      return x\n",
    "        \n",
    "model = NewArch()        \n",
    "# for n, p in model.named_parameters():\n",
    "#   print(f'Layers names: {n}')\n",
    "# print(f'modules: {model.modules}')\n",
    "# for idx, m in enumerate(model.named_modules()):\n",
    "#         print(idx, '->', m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15260/3473302607.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Module_LSTM'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# for n, p in model.named_parameters():\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#    print(f'Layers names: {n}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# print(f'Model state_dict {model.state_dict()}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.add_module('Module_LSTM', torch.nn.LSTM(10, 2))  \n",
    "# for n, p in model.named_parameters():\n",
    "#    print(f'Layers names: {n}')\n",
    "# print(f'Model state_dict {model.state_dict()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQg7n4DmpQ7w"
   },
   "source": [
    "Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Fj8rzSMw1X70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires grad: False, False\n",
      "Tensor contents: tensor([[0.4935, 0.3101, 0.9265],\n",
      "        [0.8087, 0.0660, 0.7337]]), tensor([[0.9086, 0.0583, 0.4205, 0.3383],\n",
      "        [0.0734, 0.6685, 0.6408, 0.9400],\n",
      "        [0.3415, 0.1650, 0.7830, 0.1103]])\n",
      "Leaf tensor: True, True\n",
      "Gradients: None, None\n",
      "Grad function: None, None\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand((2,3))\n",
    "t2 = torch.rand((3,4))\n",
    "print(f'Requires grad: {t1.requires_grad}, {t2.requires_grad}')\n",
    "print(f'Tensor contents: {t1}, {t2}')\n",
    "print(f'Leaf tensor: {t1.is_leaf}, {t2.is_leaf}')\n",
    "print(f'Gradients: {t1.grad}, {t2.grad}')\n",
    "print(f'Grad function: {t1.grad_fn}, {t2.grad_fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zrtLs1VQ1YEy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires grad: False\n",
      "Tensor contents: tensor([[0.7875, 0.3889, 1.1316, 0.5606],\n",
      "        [0.9902, 0.2123, 0.9569, 0.4166]])\n",
      "Leaf tensor: True\n",
      "Gradients: None\n",
      "Grad function: None\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.mm(t1,t2)\n",
    "print(f'Requires grad: {t3.requires_grad}')\n",
    "print(f'Tensor contents: {t3}')\n",
    "print(f'Leaf tensor: {t3.is_leaf}')\n",
    "print(f'Gradients: {t3.grad}')\n",
    "print(f'Grad function: {t3.grad_fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TH46W0Fg1YLc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires grad: True, True, True\n",
      "Tensor contents: tensor([[0.9330, 0.2856, 0.9335],\n",
      "        [0.8271, 0.3341, 0.2024],\n",
      "        [0.0629, 0.7649, 0.7740],\n",
      "        [0.4049, 0.7918, 0.7548]], requires_grad=True), tensor([[1.3546, 1.6643, 2.1130],\n",
      "        [1.3283, 1.4155, 2.0224]], grad_fn=<MmBackward0>), tensor([[2.0745, 1.5402, 3.2906, 2.2559],\n",
      "        [2.0015, 1.3574, 3.0492, 2.0031]], grad_fn=<MmBackward0>)\n",
      "Leaf tensor: True, False, False\n",
      "Grad function: None, <MmBackward0 object at 0x000001BDFECAEEB8>, <MmBackward0 object at 0x000001BDFECAEEB8>\n"
     ]
    }
   ],
   "source": [
    "t4 = torch.rand((4,3), requires_grad=True)\n",
    "t5 = torch.mm(t3,t4)\n",
    "t6 = torch.mm(t5,t2)\n",
    "print(f'Requires grad: {t4.requires_grad}, {t5.requires_grad}, {t6.requires_grad}')\n",
    "print(f'Tensor contents: {t4}, {t5}, {t6}')\n",
    "print(f'Leaf tensor: {t4.requires_grad}, {t5.is_leaf}, {t6.is_leaf}')\n",
    "#print(f'Gradients: {t4.grad}, {t5.grad}, {t6.grad}')\n",
    "print(f'Grad function: {t4.grad_fn}, {t5.grad_fn}, {t6.grad_fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NL38VYhcpTTm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for output = <AddBackward0 object at 0x000001BDFECBDDA0>\n",
      "Gradient function for loss = <L1LossBackward0 object at 0x000001BDF37B1240>\n",
      "Wghts gradients: tensor([[0.4367, 0.4367],\n",
      "        [0.4386, 0.4386],\n",
      "        [0.3739, 0.3739],\n",
      "        [0.1571, 0.1571],\n",
      "        [0.1078, 0.1078],\n",
      "        [0.4250, 0.4250],\n",
      "        [0.4098, 0.4098]])\n",
      "Bias gradients: tensor([0.5000, 0.5000])\n",
      "[input, label, wght, bias, output]\n",
      "[False, False, True, True, False]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "To optimize weights of parameters in the neural network, we need to compute the \n",
    "derivatives of our loss function with respect to parameters. To compute those \n",
    "derivatives, we call loss.backward(), and then retrieve the values from \n",
    "appropriate parameters by .grad attribute:\n",
    "'''\n",
    "input = torch.rand(7)\n",
    "label = torch.rand(2)\n",
    "weights = torch.rand((7,2), requires_grad=True)\n",
    "biases = torch.rand(2, requires_grad=True)\n",
    "output = torch.matmul(input, weights) + biases\n",
    "loss = torch.nn.functional.l1_loss(output, label)\n",
    "print(f'Gradient function for output = {output.grad_fn}')\n",
    "print(f'Gradient function for loss = {loss.grad_fn}')\n",
    "#loss.backward(retain_graph=True)\n",
    "loss.backward()\n",
    "print(f'Wghts gradients: {weights.grad}')\n",
    "print(f'Bias gradients: {biases.grad}')\n",
    "'''We can only obtain the grad properties for the leaf nodes of the computational graph, \n",
    "which have requires_grad property set to True\n",
    "'''\n",
    "print(f'[input, label, wght, bias, output]')\n",
    "print(list(map(lambda x: x.is_leaf and x.requires_grad, [input, label, weights, biases, output])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1638895186379,
     "user": {
      "displayName": "Andriy Sorokin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08930791557530031808"
     },
     "user_tz": -180
    },
    "id": "1Yrki3sCw5wM",
    "outputId": "7c1430f8-9c34-4972-eadc-2a436bed4826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires_grad: False\n",
      "Requires_grad: True, False\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "By default, all tensors with requires_grad=True are tracking their computational \n",
    "history and support gradient computation. However, there are some cases when we do \n",
    "not need to do that, for example, when we have trained the model and just want to \n",
    "apply it to some input data, i.e. we only want to do forward computations through the\n",
    "network. We can stop tracking computations by surrounding our computation code with \n",
    "torch.no_grad() block:\n",
    "'''\n",
    "with torch.no_grad():\n",
    "    output = torch.matmul(input, weights) + biases\n",
    "print(f'Requires_grad: {output.requires_grad}')\n",
    "\n",
    "output = torch.matmul(input, weights) + biases\n",
    "out = output.detach()\n",
    "print(f'Requires_grad: {output.requires_grad}, {out.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1638895561381,
     "user": {
      "displayName": "Andriy Sorokin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08930791557530031808"
     },
     "user_tz": -180
    },
    "id": "0CNg03lQw53k",
    "outputId": "39bbb025-d5ad-41e6-d3ff-a539ee5b2023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, False, True]\n",
      "[True, True, True, False, False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are reasons you might want to disable gradient tracking:\\nTo mark some parameters in your neural network as frozen parameters. \\nThis is a very common scenario for finetuning a pretrained network\\nTo speed up computations when you are only doing forward pass,\\nbecause computations on tensors that do not track gradients would be more efficient'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, requires_grad=True)\n",
    "b = torch.rand(3, requires_grad=True)\n",
    "c = torch.rand(3, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "  d = a + b\n",
    "  with torch.enable_grad():\n",
    "    out = c * 0.1 + d\n",
    "#print(list(map(lambda x: x.requires_grad, [a,b,c,d,out])))\n",
    "out.sum().backward()\n",
    "print(list(map(lambda x: x.requires_grad, [a,b,c,d,out])))\n",
    "\n",
    "@torch.enable_grad()\n",
    "def foo(x):\n",
    "    return x * 0.1\n",
    "with torch.no_grad():\n",
    "    d = a + b\n",
    "    out = foo(c) + d\n",
    "#out.sum().backward()\n",
    "print(list(map(lambda x: x.requires_grad, [a,b,c,d,out])))\n",
    "'''There are reasons you might want to disable gradient tracking:\n",
    "To mark some parameters in your neural network as frozen parameters. \n",
    "This is a very common scenario for finetuning a pretrained network\n",
    "To speed up computations when you are only doing forward pass,\n",
    "because computations on tensors that do not track gradients would be more efficient'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4300/1399074980.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python372\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python372\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python372\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "a = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "w1 = torch.randn((3,3), requires_grad = True)\n",
    "w2 = torch.randn((3,3), requires_grad = True)\n",
    "w3 = torch.randn((3,3), requires_grad = True)\n",
    "w4 = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "b = w1*a \n",
    "c = w2*a\n",
    "\n",
    "d = w3*b + w4*c \n",
    "\n",
    "L = (10 -d)\n",
    "\n",
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5Mh7CRHlw57z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 472.11181640625\n",
      "199 286.3227844238281\n",
      "299 175.75148010253906\n",
      "399 109.94547271728516\n",
      "499 70.78121948242188\n",
      "599 47.47263717651367\n",
      "699 33.60045623779297\n",
      "799 25.344402313232422\n",
      "899 20.430782318115234\n",
      "999 17.506406784057617\n",
      "1099 15.765946388244629\n",
      "1199 14.730096817016602\n",
      "1299 14.113605499267578\n",
      "1399 13.746692657470703\n",
      "1499 13.52832317352295\n",
      "1599 13.398356437683105\n",
      "1699 13.321005821228027\n",
      "1799 13.274969100952148\n",
      "1899 13.247570037841797\n",
      "1999 13.231263160705566\n",
      "Result: 0.004227830562740564\n",
      "Result: 0.8567936420440674\n",
      "Result: -0.0007296268595382571\n",
      "Result: -0.09334550052881241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "#device = torch.device(\"cuda:0\") \n",
    "\n",
    "'''\n",
    "We will use a problem of fitting y=sin(x) with a third order polynomial as our running example. \n",
    "The network will have four parameters, and will be trained with gradient descent to fit random data by \n",
    "minimizing the Euclidean distance between the network output and the true output.\n",
    "Polynom: a + b*x + c*x**2 + d*x**3 \n",
    "'''\n",
    "x = torch.linspace(-math.pi, math.pi, 3000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "params_num = 4\n",
    "params = [torch.randn((), device=device, dtype=dtype) for i in range(params_num)]\n",
    "# params[0] = torch.randn((), device=device, dtype=dtype)\n",
    "# params[1] = torch.randn((), device=device, dtype=dtype)\n",
    "# params[2] = torch.randn((), device=device, dtype=dtype)\n",
    "# params[3] = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "from functools import reduce\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    #y_pred = params[0] + params[1] * x + params[2] * x ** 2 + params[3] * x ** 3\n",
    "    y_pred = reduce(lambda x,y: x+y, [p*x**mod for mod, p in enumerate(params)]) \n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of parameters with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grads = [(grad_y_pred*x**mod).sum() for mod, i in enumerate(range(params_num))]\n",
    "    # params[0] = grad_y_pred.sum()\n",
    "    # params[1] = (grad_y_pred * x).sum()\n",
    "    # params[2] = (grad_y_pred * x ** 2).sum()\n",
    "    # params[3] = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    for id, p in enumerate(params):\n",
    "        p-=learning_rate * grads[id]\n",
    "        \n",
    "    # params[0] -= learning_rate * grads[0]\n",
    "    # params[1] -= learning_rate * grads[1]\n",
    "    # params[2] -= learning_rate * grads[2]\n",
    "    # params[3] -= learning_rate * grads[3]\n",
    "\n",
    "[print(f'Result: {p.item()}') for p in params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "s-L7_KC0MWhS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python372\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 6.8860931396484375\n",
      "199 6.362421989440918\n",
      "299 5.891330242156982\n",
      "399 5.4678425788879395\n",
      "499 5.087172985076904\n",
      "599 4.744746208190918\n",
      "699 4.436191558837891\n",
      "799 4.157390117645264\n",
      "899 3.904494524002075\n",
      "999 3.6739635467529297\n",
      "1099 3.462590217590332\n",
      "1199 3.2675211429595947\n",
      "1299 3.0862767696380615\n",
      "1399 2.916749954223633\n",
      "1499 2.7571985721588135\n",
      "1599 2.6062161922454834\n",
      "1699 2.462700843811035\n",
      "1799 2.325810194015503\n",
      "1899 2.1949148178100586\n",
      "1999 2.069549083709717\n",
      "2099 1.9493714570999146\n",
      "2199 1.8341312408447266\n",
      "2299 1.7236360311508179\n",
      "2399 1.6177319288253784\n",
      "2499 1.516291618347168\n",
      "2599 1.4192019701004028\n",
      "2699 1.326358675956726\n",
      "2799 1.2376633882522583\n",
      "2899 1.1530203819274902\n",
      "2999 1.0723367929458618\n",
      "Result: -0.16179263591766357\n",
      "Result: 0.7977377772331238\n",
      "Result: -0.2051025629043579\n",
      "Result: -0.084745392203331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "#device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "x = torch.linspace(-math.pi, math.pi, 3000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "params_num = 4\n",
    "params = [torch.randn((), device=device, dtype=dtype, requires_grad=True) for i in range(params_num)]\n",
    "\n",
    "from functools import reduce\n",
    "learning_rate = 1e-4\n",
    "criterion = torch.nn.MSELoss(reduce='sum')\n",
    "optim = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "for t in range(3000):\n",
    "    optim.zero_grad()\n",
    "    # Forward pass: compute predicted y\n",
    "    #y_pred = params[0] + params[1] * x + params[2] * x ** 2 + params[3] * x ** 3\n",
    "    y_pred = reduce(lambda x,y: x+y, [p*x**mod for mod, p in enumerate(params)]) \n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "   \n",
    "    # Backprop to compute gradients of parameters with respect to loss\n",
    "    loss.backward()\n",
    "    # Update weights using gradient descent\n",
    "    optim.step()\n",
    "\n",
    "[print(f'Result: {p.item()}') for p in params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XkgtoLbbMWkJ"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4300/1025799661.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;31m# By default, requires_grad=False, which indicates that we do not need to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;31m# compute gradients with respect to these Tensors during the backward pass.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python372\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "'''\n",
    "In PyTorch we can easily define our own autograd operator by defining a subclass of torch.autograd.\n",
    "Function and implementing the forward and backward functions. We can then use our new autograd operator \n",
    "by constructing an instance and calling it like a function, passing Tensors containing input data.\n",
    "Our function is 0.5 * (5 * x ** 3 - 3 * x ** 2)\n",
    "'''\n",
    "class CustomPolynomial(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate four parameters and assign them as\n",
    "        member parameters.\n",
    "        Create random Tensors for weights. For this example, we need\n",
    "        4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n",
    "        not too far from the correct result to ensure convergence.\n",
    "        Setting requires_grad=True indicates that we want to compute gradients with\n",
    "        respect to these Tensors during the backward pass.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn((), device=device, dtype=dtype, requires_grad=True)*0.01)\n",
    "        self.b = torch.nn.Parameter(torch.randn((), device=device, dtype=dtype, requires_grad=True)*0.01)\n",
    "        self.c = torch.nn.Parameter(torch.randn((), device=device, dtype=dtype, requires_grad=True)*0.01)\n",
    "        self.d = torch.nn.Parameter(torch.randn((), device=device, dtype=dtype, requires_grad=True)*0.01)\n",
    "        '''\n",
    "        P3 using our custom autograd operation.\n",
    "        To apply our Function, we use Function.apply method. We alias this as 'P3'.\n",
    "        '''\n",
    "        self.operation = CustomPolynomial.apply\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        return self.a + self.b * self.operation(self.c + self.d * x)\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "model = Polynomial3().to(device)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "sheduler = torch.optim.lr_scheduler.MultiStepLR(optim, milestones=[25000], gamma=0.1)\n",
    "\n",
    "for t in range(30000):\n",
    "    # Zero gradients in optimizer\n",
    "    optim.zero_grad()\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    y_pred =  model(x) \n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 1000 == 999:\n",
    "        print(t, loss.item())\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "    # Update weights using gradient descent\n",
    "    sheduler.step()\n",
    "    optim.step()\n",
    "\n",
    "print(model.string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yO-nhbG1MWvh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMcXX5+hGy/O0Yav4jDc4x2",
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
